# Comparison of Activation Functions for Deep Learning

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg 'Open in Colab')](https://colab.research.google.com/github/blakebollinger/compare-activation-functions/blob/main/Comparison_of_Activation_Functions_for_Neural_Networks.ipynb)

---

## Overview

This project includes a simple convolutional model that is trained on the CIFAR10 dataset. It runs the training sequence multiple times and plots the differences when using activaiton functions:
 
*   Sigmoid Function
*   ReLU Function
*   Leaky-ReLU Function

## Quick Start

Running this code in Google Colab is the easiest way to get up and running fast. Just hit the link at the top of `README.md` to get started
